{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKgiJppBOPbZ"
   },
   "source": [
    "### Performing a simple linear regression with Tensorflow\n",
    "From class room week 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7UMiqKnkOOQT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.set_printoptions(suppress=True) #determine the way floating points, \n",
    "#numbers, arrays and other NumPy objects are displayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2le8oBvOUe5"
   },
   "source": [
    "## Download the MNIST dataset using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "7OJUV960Itp6",
    "outputId": "093460b8-c545-4160-cb5c-84eba9afb416",
    "tags": []
   },
   "outputs": [],
   "source": [
    "(x_all, y_all),(x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "id": "WUWsusa0I2hz",
    "outputId": "3eb23259-d5d3-4fe4-88c3-a54b8f193160",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_data in module keras.datasets.mnist:\n",
      "\n",
      "load_data(path='mnist.npz')\n",
      "    Loads the MNIST dataset.\n",
      "    \n",
      "    This is a dataset of 60,000 28x28 grayscale images of the 10 digits,\n",
      "    along with a test set of 10,000 images.\n",
      "    More info can be found at the\n",
      "    [MNIST homepage](http://yann.lecun.com/exdb/mnist/).\n",
      "    \n",
      "    Args:\n",
      "      path: path where to cache the dataset locally\n",
      "        (relative to `~/.keras/datasets`).\n",
      "    \n",
      "    Returns:\n",
      "      Tuple of NumPy arrays: `(x_train, y_train), (x_test, y_test)`.\n",
      "    \n",
      "    **x_train**: uint8 NumPy array of grayscale image data with shapes\n",
      "      `(60000, 28, 28)`, containing the training data. Pixel values range\n",
      "      from 0 to 255.\n",
      "    \n",
      "    **y_train**: uint8 NumPy array of digit labels (integers in range 0-9)\n",
      "      with shape `(60000,)` for the training data.\n",
      "    \n",
      "    **x_test**: uint8 NumPy array of grayscale image data with shapes\n",
      "      (10000, 28, 28), containing the test data. Pixel values range\n",
      "      from 0 to 255.\n",
      "    \n",
      "    **y_test**: uint8 NumPy array of digit labels (integers in range 0-9)\n",
      "      with shape `(10000,)` for the test data.\n",
      "    \n",
      "    Example:\n",
      "    \n",
      "    ```python\n",
      "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
      "    assert x_train.shape == (60000, 28, 28)\n",
      "    assert x_test.shape == (10000, 28, 28)\n",
      "    assert y_train.shape == (60000,)\n",
      "    assert y_test.shape == (10000,)\n",
      "    ```\n",
      "    \n",
      "    License:\n",
      "      Yann LeCun and Corinna Cortes hold the copyright of MNIST dataset,\n",
      "      which is a derivative work from original NIST datasets.\n",
      "      MNIST dataset is made available under the terms of the\n",
      "      [Creative Commons Attribution-Share Alike 3.0 license.](\n",
      "      https://creativecommons.org/licenses/by-sa/3.0/)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.keras.datasets.mnist.load_data) # description of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "oew1UUP7I80b",
    "outputId": "fe2df7c2-8625-4144-ef06-b156bc99e710",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_all samples = (60000, 28, 28)\n",
      "y_all samples = (60000,)\n",
      "x_test samples = (10000, 28, 28)\n",
      "y_test samples = (10000,)\n"
     ]
    }
   ],
   "source": [
    "print ('x_all samples = ' + str(x_all.shape))\n",
    "print ('y_all samples = ' + str(y_all.shape))\n",
    "\n",
    "print ('x_test samples = ' + str(x_test.shape))\n",
    "print ('y_test samples = ' + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmWdkq80Od_c"
   },
   "source": [
    "## Preprocess the data ready for tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zMa_0RUfJSV1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_input = tf.keras.utils.to_categorical(y_all)      #one hot encoder\n",
    "x_input = (np.reshape(x_all, (x_all.shape[0], 784)) / 255.0).astype(np.float32)    #flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you train on only 50000 images and keep 10000 images as validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train samples = (50000, 784)\n",
      "y_train samples = (50000, 10)\n",
      "x_validate samples = (10000, 784)\n",
      "y_validate samples = (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "x_validate = x_input[0:10000,:]\n",
    "y_validate = y_input[0:10000,:]\n",
    "x_train = x_input[10000:,:]\n",
    "y_train = y_input[10000:,:]\n",
    "\n",
    "# we'll print out the shapes of the arrays to check it's what we expect\n",
    "print ('x_train samples = ' + str(x_train.shape))\n",
    "print ('y_train samples = ' + str(y_train.shape))\n",
    "print ('x_validate samples = ' + str(x_validate.shape))\n",
    "print ('y_validate samples = ' + str(y_validate.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "x_test = (np.reshape(x_test, (x_test.shape[0], 784)) / 255.0).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "wE9YY6maJtXk",
    "outputId": "c374f2d5-f9ec-4182-edfa-5a053a9cda27",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test samples = (10000, 784)\n",
      "y_test samples = (10000, 10, 2)\n"
     ]
    }
   ],
   "source": [
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "x_test = (np.reshape(x_test, (x_test.shape[0], 784)) / 255.0).astype(np.float32)\n",
    "print ('x_test samples = ' + str(x_test.shape))\n",
    "print ('y_test samples = ' + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_eRHO2OOjp3"
   },
   "source": [
    "## Create the tensorflow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Kx9m8VH_J-T1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 20:26:00.814176: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-16 20:26:01.330968: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14660 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0001:00:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sssh2qugOpE5"
   },
   "source": [
    "## Set-up the multinomial logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6en-2wR-KU20",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predicted probability for each class\n",
    "def y_pred(x):\n",
    "    return tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "\n",
    "# cross entropy loss function\n",
    "@tf.function\n",
    "def loss(x,y):\n",
    "    y_ = y_pred(x)\n",
    "    return tf.reduce_mean(-tf.reduce_sum(y * tf.math.log(y_), axis=[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# New code for calculating validation accuracy\n",
    "def accuracy(x,y):\n",
    "    # compare the prediction to the label, if they're the same\n",
    "    # tf.equal will return 1, if they're different it will be 0\n",
    "    y_ = y_pred(x)\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    # calculate the accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hj3YquijPcVR"
   },
   "source": [
    "## Set-up the training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "OPOgRIiAK5ZY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_steps = 500\n",
    "\n",
    "# learning rate\n",
    "lr = 1e-2\n",
    "\n",
    "# gradient descent optimizer\n",
    "optimizer = tf.optimizers.SGD(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8V5f36yPfYH"
   },
   "source": [
    "## Create a session and run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "ILHQOx0kLcLF",
    "outputId": "7db595ac-8212-4768-f0bf-58e034e1ad60",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up logging.\n",
    "logdir = '/home/jovyan/logs'\n",
    "writer = tf.summary.create_file_writer(logdir)\n",
    "\n",
    "with writer.as_default(): # use the contex manager\n",
    "    for i in range(train_steps):\n",
    "        with tf.GradientTape() as tape:\n",
    "            current_loss = loss(x_input,y_input)\n",
    "        gradients = tape.gradient(current_loss, [W, b])\n",
    "        optimizer.apply_gradients(zip(gradients, [W ,b]))\n",
    "        # calculate the validation accuracy\n",
    "        val_acc = accuracy(x_validate,y_validate)\n",
    "        # write the value to tensorboard\n",
    "        tf.summary.scalar('val_acc', val_acc, step=i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "shallow_mnist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
