{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - Week 3\n",
    "\n",
    "## Modify the MNIST classifier so that instead of training on the full data set, you train on only 50000 images and keep 10000 images as validation data. Add operations that will calculate the accuracy of the classifier on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7UMiqKnkOOQT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load the necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.set_printoptions(suppress=True) #determine the way floating point \n",
    "#numbers, arrays and other NumPy objects are displayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2le8oBvOUe5"
   },
   "source": [
    "## Download the MNIST dataset using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "7OJUV960Itp6",
    "outputId": "093460b8-c545-4160-cb5c-84eba9afb416",
    "tags": []
   },
   "outputs": [],
   "source": [
    "(x_all, y_all),(x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "id": "WUWsusa0I2hz",
    "outputId": "3eb23259-d5d3-4fe4-88c3-a54b8f193160",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#help(tf.keras.datasets.mnist.load_data) # description of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "oew1UUP7I80b",
    "outputId": "fe2df7c2-8625-4144-ef06-b156bc99e710",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_all samples = (60000, 28, 28)\n",
      "y_all samples = (60000,)\n",
      "x_test samples = (10000, 28, 28)\n",
      "y_test samples = (10000,)\n"
     ]
    }
   ],
   "source": [
    "print ('x_all samples = ' + str(x_all.shape))\n",
    "print ('y_all samples = ' + str(y_all.shape))\n",
    "\n",
    "print ('x_test samples = ' + str(x_test.shape))\n",
    "print ('y_test samples = ' + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmWdkq80Od_c"
   },
   "source": [
    "## Preprocess the data ready for tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zMa_0RUfJSV1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_input = tf.keras.utils.to_categorical(y_all)      #one hot encoder\n",
    "x_input = (np.reshape(x_all, (x_all.shape[0], 784)) / 255.0).astype(np.float32)    #flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_input samples = (60000, 784)\n",
      "y_input samples = (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "#you train on only 50000 images and keep 10000 images as validation data.\n",
    "print ('x_input samples = ' + str(x_input.shape))\n",
    "print ('y_input samples = ' + str(y_input.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train samples = (50000, 784)\n",
      "y_train samples = (50000, 10)\n",
      "x_validate samples = (10000, 784)\n",
      "y_validate samples = (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "x_validate = x_input[0:10000,:] #the first 10000\n",
    "y_validate = y_input[0:10000,:]\n",
    "x_train = x_input[10000:,:]\n",
    "y_train = y_input[10000:,:]\n",
    "\n",
    "# we'll print out the shapes of the arrays to check it's what we expect\n",
    "print ('x_train samples = ' + str(x_train.shape))\n",
    "print ('y_train samples = ' + str(y_train.shape))\n",
    "print ('x_validate samples = ' + str(x_validate.shape))\n",
    "print ('y_validate samples = ' + str(y_validate.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "x_test = (np.reshape(x_test, (x_test.shape[0], 784)) / 255.0).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "wE9YY6maJtXk",
    "outputId": "c374f2d5-f9ec-4182-edfa-5a053a9cda27",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test samples = (10000, 784)\n",
      "y_test samples = (10000, 10, 2)\n"
     ]
    }
   ],
   "source": [
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "x_test = (np.reshape(x_test, (x_test.shape[0], 784)) / 255.0).astype(np.float32)\n",
    "print ('x_test samples = ' + str(x_test.shape))\n",
    "print ('y_test samples = ' + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_eRHO2OOjp3"
   },
   "source": [
    "## Create the tensorflow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Kx9m8VH_J-T1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-21 20:33:31.791705: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-21 20:33:32.487435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7426 MB memory:  -> device: 0, name: Tesla M60, pci bus id: 0001:00:00.0, compute capability: 5.2\n"
     ]
    }
   ],
   "source": [
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sssh2qugOpE5"
   },
   "source": [
    "## Set-up the multinomial logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6en-2wR-KU20",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predicted probability for each class\n",
    "def y_pred(x):\n",
    "    return tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "\n",
    "# cross entropy loss function\n",
    "@tf.function\n",
    "def loss(x,y):\n",
    "    y_ = y_pred(x)\n",
    "    return tf.reduce_mean(-tf.reduce_sum(y * tf.math.log(y_), axis=[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# New code for calculating validation accuracy\n",
    "def accuracy(x,y):\n",
    "    # compare the prediction to the label, if they're the same\n",
    "    # tf.equal will return 1, if they're different it will be 0\n",
    "    y_ = y_pred(x)\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    # calculate the accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hj3YquijPcVR"
   },
   "source": [
    "## Set-up the training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "OPOgRIiAK5ZY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_steps = 500\n",
    "\n",
    "# learning rate\n",
    "lr = 1e-2\n",
    "\n",
    "# gradient descent optimizer\n",
    "optimizer = tf.optimizers.SGD(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8V5f36yPfYH"
   },
   "source": [
    "## Create a session and run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "ILHQOx0kLcLF",
    "outputId": "7db595ac-8212-4768-f0bf-58e034e1ad60",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up logging.\n",
    "logdir = '/home/jovyan/logs'\n",
    "writer = tf.summary.create_file_writer(logdir)\n",
    "\n",
    "with writer.as_default(): # use the contex manager\n",
    "    for i in range(train_steps):\n",
    "        with tf.GradientTape() as tape:\n",
    "            current_loss = loss(x_input,y_input)\n",
    "        gradients = tape.gradient(current_loss, [W, b])\n",
    "        optimizer.apply_gradients(zip(gradients, [W ,b]))\n",
    "        # calculate the validation accuracy\n",
    "        val_acc = accuracy(x_validate,y_validate)\n",
    "        # write the value to tensorboard\n",
    "        tf.summary.scalar('val_acc', val_acc, step=i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "shallow_mnist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
